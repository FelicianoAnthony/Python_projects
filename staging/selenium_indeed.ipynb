{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import os\n",
    "import time\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "    ''' create bs4 object '''\n",
    "    \n",
    "    r = requests.get(url, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36\"\n",
    "    })\n",
    "    return BeautifulSoup(r.content, \"html5lib\")\n",
    "\n",
    "def next_page_url(url):\n",
    "    ''' returns the url for the next page on an indeed job search page '''\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # this creates a list of \"Results Page\" at bottom of screen... last url will always be next page \n",
    "    next_pages_urls = []\n",
    "    for i in soup.find_all(attrs={'class': 'pagination'}):\n",
    "        a_tags = i.find_all('a')\n",
    "        for a in a_tags:\n",
    "            next_pages = 'https://www.indeed.com' + a['href']\n",
    "            next_pages_urls.append(next_pages)\n",
    "\n",
    "    return next_pages_urls[-1]\n",
    "\n",
    "\n",
    "def scrape_job_links(url):\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # get hrefs ONLY for non- header and footer job postings \n",
    "    lst1= []\n",
    "    for div in soup.find_all('div', {'class':' row result'}):\n",
    "        links = div.find('a')['href']\n",
    "        #print(links)\n",
    "        lst1.append(links)\n",
    "\n",
    "    # last row has different class name\n",
    "    for div in soup.find_all('div', {'class':'lastRow row result'}):\n",
    "        links = div.find('a')['href']\n",
    "        #print(div)\n",
    "        lst1.append(links)\n",
    "    \n",
    "    links = ['https://www.indeed.com' + i for i in lst1]\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bottom_scroll(webdriver):\n",
    "    ''''scroll to bottom of page--- w/ page search pages -- not individual job postings'''\n",
    "    element=webdriver.find_element_by_xpath('//*[@id=\"resultsCol\"]/div[18]')\n",
    "    return element.location_once_scrolled_into_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indeed_scraper(): \n",
    "    '''this combined everything '''\n",
    "    \n",
    "\n",
    "    driver = setup_webdriver()\n",
    "    \n",
    "    # set website to scrape and query \n",
    "    driver.get(\"https://www.indeed.com/\")\n",
    "    driver.find_element_by_id('what').clear()\n",
    "    driver.find_element_by_id('what').send_keys('python developer')\n",
    "    driver.find_element_by_id('where').clear()\n",
    "    driver.find_element_by_id('where').send_keys('New York, NY')\n",
    "    driver.find_element_by_id('fj').click()\n",
    "\n",
    "    \n",
    "    # x out advertisement \n",
    "    driver.find_element_by_id('prime-popover-x').click()\n",
    "    \n",
    "    # get current url and pass into bs4\n",
    "    this_url = driver.current_url\n",
    "    date_soup = create_soup(this_url)\n",
    "    \n",
    "    # sort by date \n",
    "    date_button = date_soup.find(attrs={'class': 'no-wrap'})\n",
    "    date_button2 = date_button.find('a')['href']\n",
    "    sort_by_date = 'https://www.indeed.com' + date_button2\n",
    "    \n",
    "    # pass url to driver & get current url to scrape links \n",
    "    driver.get(sort_by_date)\n",
    "    \n",
    "    # scrape em\n",
    "    all_job_links = []\n",
    "    for i in range(5):\n",
    "        # get current url\n",
    "        driver.get(driver.current_url)\n",
    "        \n",
    "        # get job links & append to list\n",
    "        job_links = scrape_job_links(driver.current_url)\n",
    "        all_job_links.extend(job_links)\n",
    "        \n",
    "        # scroll to the bottom of screen\n",
    "        bottom_scroll(driver)\n",
    "        \n",
    "        # go to next page\n",
    "        url_next = next_page_url(driver.current_url)\n",
    "        driver.get(url_next)\n",
    "    return all_job_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs = indeed_scraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# need to figure out \n",
    "def filter_links(links_lst):\n",
    "    \n",
    "    \n",
    "    indeed_links = []\n",
    "    for i in links_lst:\n",
    "        if i.startswith('https://www.indeed.com/rc'):\n",
    "            pass\n",
    "        else:\n",
    "            #time.sleep(2)\n",
    "            #webdriver_obj.get(i) #!!!!!! CHANGE THIS\n",
    "            indeed_links.append(i)\n",
    "    return indeed_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs_filt = filter_links(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scroll(webdriver):\n",
    "    \n",
    "    # scroll to bottom of page\n",
    "    element=webdriver.find_element_by_xpath('//*[@id=\"resultsCol\"]/div[18]')\n",
    "    return element.location_once_scrolled_into_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_scrolling():\n",
    "\n",
    "    driver= setup_webdriver()\n",
    "    \n",
    "    # get specifics \n",
    "    driver.get(\"https://www.indeed.com/\")\n",
    "    driver.find_element_by_id('what').clear()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('what').send_keys('python developer')\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('where').clear()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('where').send_keys('New York, NY')\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('fj').click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    driver.find_element_by_id('prime-popover-x').click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # scrape jobs from that url \n",
    "    job_links = scrape_job_links(driver.current_url)\n",
    "    # filter to only indeed jobs\n",
    "    jobs_filt = filter_links(job_links)\n",
    "    \n",
    "    # start at a good xpath and count up div tags that appear in every page to 'smooth' scroll\n",
    "    count = 0\n",
    "    for i in range(2):\n",
    "        for j in jobs_filt:\n",
    "            driver.get(j)\n",
    "            time.sleep(2)\n",
    "            count+=1\n",
    "            #print(count, j)\n",
    "            str_ = str(count)\n",
    "\n",
    "            #concat_div = '//*[@id=\"resultsCol\"]/div[' + str_ + ']'\n",
    "            try: \n",
    "                concat_div = '//*[@id=\"job-content\"]/tbody/tr/td[1]/table/tbody/tr/td/div[2]/div[2]/div[' + str_ + ']'\n",
    "                #concat_div = '//*[@id=\"job-content\"]/tbody/tr/td[1]/table/tbody/tr/td/div[2]/div[' + str_ + ']'\n",
    "                element=driver.find_element_by_xpath(concat_div)\n",
    "                element.location_once_scrolled_into_view\n",
    "                time.sleep(2)\n",
    "                driver.get(j)\n",
    "                print(j)\n",
    "            except:\n",
    "                browser.get(j)\n",
    "                print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/company/NYTP/jobs/Python-Engineer-8578a5b36c166821?fccid=ccff84ac3dd19bb9\n",
      "https://www.indeed.com/company/RAPS-consulting/jobs/Senior-Python-Developer-21d5e8cd39ab722f?fccid=9b7a3860892e2fcd\n",
      "https://www.indeed.com/company/Kasisto/jobs/Front-End-Software-Engineer-e8ee0083bfbe3831?fccid=521ca7ccc49e32d5\n",
      "https://www.indeed.com/company/Chase-Dream-LLC/jobs/Software-Engineer-Python-98563184e467ab84?fccid=ed0eadb761b45e00\n",
      "https://www.indeed.com/company/LIS-Solutions/jobs/Python-Developer-a424fc091c993864?fccid=58aef85eed32eea8\n",
      "https://www.indeed.com/company/BlindData/jobs/Software-Engineer-010d7c76f5d1fa19?fccid=55962a85574fa94f\n",
      "https://www.indeed.com/company/NYTP/jobs/Python-Engineer-8578a5b36c166821?fccid=ccff84ac3dd19bb9\n",
      "https://www.indeed.com/company/RAPS-consulting/jobs/Senior-Python-Developer-21d5e8cd39ab722f?fccid=9b7a3860892e2fcd\n",
      "https://www.indeed.com/company/Kasisto/jobs/Front-End-Software-Engineer-e8ee0083bfbe3831?fccid=521ca7ccc49e32d5\n",
      "https://www.indeed.com/company/Chase-Dream-LLC/jobs/Software-Engineer-Python-98563184e467ab84?fccid=ed0eadb761b45e00\n",
      "https://www.indeed.com/company/LIS-Solutions/jobs/Python-Developer-a424fc091c993864?fccid=58aef85eed32eea8\n",
      "https://www.indeed.com/company/BlindData/jobs/Software-Engineer-010d7c76f5d1fa19?fccid=55962a85574fa94f\n"
     ]
    }
   ],
   "source": [
    "smooth_scrolling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver=setup_webdriver()\n",
    "driver.get(jobs[0])\n",
    "\n",
    "\n",
    "lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "match=False\n",
    "while(match==False):\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(3)\n",
    "        lenOfPage = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        if lastCount==lenOfPage:\n",
    "            match=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
