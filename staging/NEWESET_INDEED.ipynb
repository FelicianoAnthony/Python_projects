{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import urllib.request\n",
    "import requests\n",
    "import sqlite3\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "Instance = None\n",
    "\n",
    "def Initialize():\n",
    "    global Instance\n",
    "    Instance = webdriver.Chrome('/Users/Anthony/Desktop/chromedriver')\n",
    "    Instance.implicitly_wait(5)\n",
    "    return Instance\n",
    "\n",
    "def CloseDriver():\n",
    "    global Instance\n",
    "    Instance.quit()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_webdriver(path_to_driver): \n",
    "    '''set up webdriver'''\n",
    "\n",
    "    chromedriver = path_to_driver\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    return driver   \n",
    "\n",
    "\n",
    "def create_soup(url):\n",
    "    ''' create bs4 object '''\n",
    "    r = urllib.request.urlopen(url).read()\n",
    "    return  BeautifulSoup(r, \"lxml\")\n",
    "\n",
    "\n",
    "def scrape_job_post_links(url):\n",
    "    ''' returns all links from a query on indeed mobile '''\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "\n",
    "    # get employers -- br tags\n",
    "    h2 = soup.find('h2')\n",
    "    company = [h.next_element.next_element.next_element\n",
    "           for h in h2.find_all_next('a')][0:10]\n",
    "    company_stripped = [i.strip().replace(' -', '') for i in company]\n",
    "\n",
    "    # get job title and job url\n",
    "    job_title = []\n",
    "    job_url = []\n",
    "    for h2 in soup.find_all('h2', {'class': 'jobTitle'}):\n",
    "        job = h2.text\n",
    "        job_title.append(job)\n",
    "        for a in h2:\n",
    "            concat_urls = 'https://www.indeed.com/m/' + a['href']\n",
    "            job_url.append(concat_urls)\n",
    "\n",
    "    # turn unique identifiers into dictionary of tuples\n",
    "    jobs_dict = dict(zip(job_url, zip(job_title, company_stripped)))\n",
    "\n",
    "    return jobs_dict\n",
    "\n",
    "\n",
    "def iterate_job_pages(driver, int_pages_to_scrape, show_uid_dict=None):\n",
    "    '''returns a list of job links'''\n",
    "    \n",
    "    jobs_dict = {}\n",
    "    \n",
    "    if len(driver.current_url.split('=')) > 3:\n",
    "        \n",
    "        # get all links & extract url key\n",
    "        uids_dict = scrape_job_post_links(driver.current_url)\n",
    "        \n",
    "        # store uids from 1st page in new dict\n",
    "        jobs_dict = uids_dict\n",
    "        \n",
    "        # extend and next page \n",
    "        driver.find_element_by_xpath('/html/body/p[22]/a').click()\n",
    "\n",
    "    page_count = 0\n",
    "\n",
    "    for x in range(int(int_pages_to_scrape)):\n",
    "        page_count+=1\n",
    "        last_date = [i.text for i in driver.find_elements_by_class_name('date')][-1]\n",
    "        print('Last date on page', page_count, 'of', int_pages_to_scrape, '==>', last_date)\n",
    "\n",
    "        # page urls is a diuctionary \n",
    "        uids_dict2 = scrape_job_post_links(driver.current_url)\n",
    "\n",
    "        # store uids from every page thats not 1 in same dict\n",
    "        jobs_dict.update(uids_dict2)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        driver.find_element_by_xpath('/html/body/p[22]/a[2]').click()\n",
    "        \n",
    "    return jobs_dict\n",
    "\n",
    "\n",
    "def create_db_table(full_path_to_db):\n",
    "    ''' create database table & names columns '''\n",
    "\n",
    "    conn = sqlite3.connect(full_path_to_db)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE indeed_jobs\n",
    "        (id integer primary key, data,\n",
    "        url text, \n",
    "        company_name text, \n",
    "        job_title text,\n",
    "        date_added timestamp )''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "def check_db_for_job(uid_dict, path_to_db, new_db=None):\n",
    "    \n",
    "    if new_db:\n",
    "        create_db_table(path_to_db)\n",
    "    \n",
    "    conn = sqlite3.connect(path_to_db)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    # turn dictionary into tuple \n",
    "    tup = [(k, v[0], v[1]) for k,v in uid_dict.items()]\n",
    "    \n",
    "    print('Check database for duplicates...')\n",
    "    new_urls = []\n",
    "    for job_url, job_title, job_company in tup:\n",
    "\n",
    "        c.execute('SELECT * FROM indeed_jobs WHERE (url=? AND company_name=? AND job_title=? AND date_added=?)', (job_url, job_company, job_title, datetime.now().date()))\n",
    "        entry = c.fetchone()\n",
    "\n",
    "        if entry is None:\n",
    "            c.execute(\"insert or ignore into indeed_jobs (url, company_name, job_title, date_added) values (?, ?, ?, ?)\",\n",
    "                (job_url, job_company, job_title, datetime.now().date()))\n",
    "            conn.commit()\n",
    "            \n",
    "            new_urls.append(job_url)\n",
    "            \n",
    "            print ('\\n', 'New entry added', '\\n', job_title.encode(\"utf-8\"), job_company.encode(\"utf-8\"), '\\n')\n",
    "        \n",
    "        else:\n",
    "            print ('Entry found')\n",
    "            \n",
    "    return new_urls\n",
    "    \n",
    "    \n",
    "def indeed_scraper(path_to_driver, job_name, job_location, pages_to_search_int, db_path, sleep_int,\n",
    "                   new_db=False):\n",
    "     \n",
    "    driver = setup_webdriver(path_to_driver)\n",
    " \n",
    "    driver.get('https://indeed.com/m/')\n",
    "    driver.find_element_by_xpath('/html/body/form/p[1]/input').clear()\n",
    "    driver.find_element_by_xpath('/html/body/form/p[1]/input').send_keys(job_name)\n",
    "    driver.find_element_by_xpath('/html/body/form/p[2]/input').clear()\n",
    "    driver.find_element_by_xpath('/html/body/form/p[2]/input').send_keys(job_location)\n",
    "    driver.find_element_by_xpath('/html/body/form/p[3]/input').click()\n",
    " \n",
    "    driver.get(driver.current_url  +  '&sort=date')\n",
    "    \n",
    "    # returns a dictionary \n",
    "    job_dict = iterate_job_pages(driver, pages_to_search_int)\n",
    "    \n",
    "    # return a list \n",
    "    unique_urls = check_db_for_job(job_dict, db_path, new_db=new_db)   \n",
    "    len_urls = len(unique_urls)\n",
    "    minutes = len_urls * int(sleep_int) / 60\n",
    "    \n",
    "    \n",
    "    msg = '{} {} {} {} {}'.format('There are',  len_urls,  \n",
    "                              'urls and this is going to take', minutes, \n",
    "                              'minutes to view. Break it up into chunks by entering number. Enter n to skip.  ')\n",
    "    \n",
    "    ques = input(msg)\n",
    "    if ques == 'n':\n",
    "        for u in unique_urls:\n",
    "            concat_url = 'window.open(\"' + u + '\",\"_blank\");'\n",
    "            driver.execute_script(concat_url)\n",
    "            time.sleep(int(sleep_int))\n",
    "    else:\n",
    "        count = 0\n",
    "        run_loop = True\n",
    "        while run_loop:\n",
    "            for u in unique_urls:\n",
    "                count+=1\n",
    "                concat_url = 'window.open(\"' + u + '\",\"_blank\");'\n",
    "                driver.execute_script(concat_url)\n",
    "                time.sleep(int(sleep_int))\n",
    "                if count % int(ques) ==0:\n",
    "                    print(count, 'of', len_urls)\n",
    "                    q = input('Press enter to continue')\n",
    "                if count == len_urls:\n",
    "                    run_loop=False\n",
    "                    input('Press enter to CLOSE CURRENT WEBDRIVER WINDOW')\n",
    "                    \n",
    "def applied_jobs(url, full_path_to_db, new_db=False):\n",
    "    ''' create db of jobs already applied to -- use new_db if 1st time creating db '''\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    p = soup.find('p')\n",
    "    \n",
    "    company = [i.next_element for i in p.find_all_next('br')][0]\n",
    "    company_stripped = name.strip().replace(' -', '')\n",
    "\n",
    "    job_name = soup.find('font', {'size': '+1'}).text\n",
    "    \n",
    "    if new_db:\n",
    "        create_db_table(full_path_to_db)\n",
    "        \n",
    "    conn = sqlite3.connect(full_path_to_db)\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT * FROM indeed_jobs WHERE (url=? AND company_name=? AND job_title=?)', (url, company_stripped, job_name))\n",
    "    entry = c.fetchone()\n",
    "    \n",
    "    if entry is None:\n",
    "        c.execute(\"insert or ignore into indeed_jobs (url, company_name, job_title) values (?, ?, ?)\",\n",
    "            (url, company_stripped, job_name))\n",
    "        conn.commit()\n",
    "        print ('\\n', 'New entry added', '\\n', company_stripped.encode(\"utf-8\"), job_name.encode(\"utf-8\"), '\\n')\n",
    "\n",
    "    else:\n",
    "        print ('Entry found')\n",
    "\n",
    "def ask_questions():\n",
    "    add_or_search = input('Do you want to search for jobs or add job applied to into database? search/add   ')\n",
    "    if add_or_search == 'search':\n",
    "        job_name = input('Enter a job name.  ')\n",
    "        job_location = input('Enter a City, State location.  ')\n",
    "        pages_to_scrape = input('Enter the number of pages to scrape.  ')\n",
    "        sleep_int = input('Enter amount of time alloted to read job posting (in seconds).  ')\n",
    "        db_path = input('Enter FULL PATH to folder you want database (must end with .sqlite).  ')\n",
    "        driver_path = input('Enter FULL PATH to webdriver.  ')\n",
    "        new_db = input('Is this a new database? y/n  ')\n",
    "\n",
    "        if new_db == 'y':\n",
    "            indeed_scraper(driver_path, job_name, job_location, pages_to_scrape, db_path, sleep_int, new_db=new_db)\n",
    "        else:\n",
    "            indeed_scraper(driver_path, job_name, job_location, pages_to_scrape, db_path, sleep_int)\n",
    "\n",
    "    if add_or_search == 'add':\n",
    "        url = input('Enter the url you would like to add to database.  ')\n",
    "        db_path_applied = input('Enter FULL PATH to folder you want database (must end with .sqlite).  ')\n",
    "        new_db_applied = input('Is this a new database? y/n  ')\n",
    "        if new_db_applied == 'y':\n",
    "            applied_jobs(url, db_path_applied, new_db=new_db_applied)\n",
    "        else:\n",
    "            applied_jobs(url, db_path_applied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver_path = 'C:\\\\Users\\\\Anthony\\\\Desktop\\\\chromedriver.exe'\n",
    "db_path = 'C:\\\\Users\\\\Anthony\\\\Documents\\\\DB\\\\indeed66.sqlite'\n",
    "job_name = 'python developer'\n",
    "job_location = 'new york, ny'\n",
    "pages_to_search = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to search for jobs or add job applied to into database? search/add   search\n",
      "Enter a job name.  python developer\n",
      "Enter a City, State location.  new york, ny\n",
      "Enter the number of pages to scrape.  2\n",
      "Enter amount of time alloted to read job posting (in seconds).  2\n",
      "Enter FULL PATH to folder you want database (must end with .sqlite).  C:\\\\Users\\\\Anthony\\\\Documents\\\\DB\\\\indeed33.sqlite\n",
      "Enter FULL PATH to webdriver.  C:\\\\Users\\\\Anthony\\\\Desktop\\\\chromedriver.exe\n",
      "Is this a new database? y/n  y\n",
      "Last date on page 1 of 2 ==> 16 hours ago\n",
      "Last date on page 2 of 2 ==> 14 hours ago\n",
      "Check database for duplicates...\n",
      "\n",
      " New entry added \n",
      " b'QE-GG2-Technical Engineer' b'SQS Group' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Chatbot Developer Intern Fall 2017' b'Fresh Digital Group' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Front End Developer' b'Smith & Keller' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Sr. Software Engineer - Java Development' b'Tenable' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Applications Bioinformatician' b'Oxford Nanopore Technologies' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'C++ Software Engineer' b'Harris Allied' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Open Source Engineer (Golang or Python)' b'Capital Markets Placement' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Technology - Finance & Risk Engineering - Developer_NYC, NY' b'Silicon Staff Inc' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Customer Success Engineer' b'Parse.ly' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Python Developer' b'Smith & Keller' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Software Engineer' b'Searchlight Inc' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Software Developer' b'Harrington Starr' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'SAS Programmer' b'Origin To Future' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Software Engineer in Test' b'MediaMath' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Associate Director, Data & Analytics' b'Carat' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Dev Ops Engineer' b'PRI Technology' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Ruby on Rails Developer' b'Workbridge Associates' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Reactjs Developer' b'New York Technology Partners' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Systems Software Engineer' b'Twitter' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Machine Learning Developer' b'JP Morgan Chase' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'E-trading Java - Software Engineer' b'JP Morgan Chase' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'DevOps Engineer (Docker)' b'JMD Partners Inc' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Infrastructure Developer - Public Cloud Enablement Services' b'JP Morgan Chase' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Data Scientist' b'DigitalOcean' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Senior Data Engineer - Fintech & Real Estate Startup - New Y...' b'Twenty Recruitment' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Controls and Cyber Security Developer' b'JP Morgan Chase' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Site Reliability Engineer' b'Tumblr' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Model Validation Derivative Pricing Validator' b'Citi' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Python/Django Developer' b'Smith & Keller' \n",
      "\n",
      "\n",
      " New entry added \n",
      " b'Hadoop/Spark developer' b'VDart Inc' \n",
      "\n",
      "There are 30 urls and this is going to take 1.0 minutes to view. Break it up into chunks by entering number. Enter n to skip.  25\n",
      "25 of 30\n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "ask_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C:\\\\Users\\\\Anthony\\\\Desktop\\\\chromedriver.exe \n",
    "C:\\\\Users\\\\Anthony\\\\Documents\\\\DB\\\\indeed35.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last date on page 1 of 3 ==> 1 hour ago\n",
      "Last date on page 2 of 3 ==> 1 hour ago\n",
      "Last date on page 3 of 3 ==> 2 hours ago\n"
     ]
    }
   ],
   "source": [
    "driver = setup_webdriver(driver_path)\n",
    "\n",
    "driver.get('https://indeed.com/m/')\n",
    "driver.find_element_by_xpath('/html/body/form/p[1]/input').clear()\n",
    "driver.find_element_by_xpath('/html/body/form/p[1]/input').send_keys('python')\n",
    "driver.find_element_by_xpath('/html/body/form/p[2]/input').clear()\n",
    "driver.find_element_by_xpath('/html/body/form/p[2]/input').send_keys('new york')\n",
    "driver.find_element_by_xpath('/html/body/form/p[3]/input').click()\n",
    "\n",
    "driver.get(driver.current_url  +  '&sort=date')\n",
    "\n",
    "# returns a dictionary \n",
    "job_dict = iterate_job_pages(driver, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# working version                 \n",
    "def query_job_posting(url, query_list_include, query_list_exclude): \n",
    "    ''' |QUERIES MUST BE LOWERCASE|\n",
    "        query_list = turns job post to list of words & if any word in list match, return url \n",
    "        query_phrase_as_str = search job post as 1 big list for phrases '''\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    desc = soup.find_all('div', {'id': 'desc'})\n",
    "    \n",
    "    \n",
    "    desc_lol = [i.get_text().lower().split() for i in desc]\n",
    "    desc_flattened = [inner for outer in desc_lol for inner in outer]\n",
    "    desc_regex = [(re.sub('[^A-Za-z0-9]+', ' ', i)) for i in desc_flattened]\n",
    "    \n",
    "    split_after_regex = []\n",
    "    for i in desc_regex:\n",
    "        split_after_regex.extend(i.split())\n",
    "        \n",
    "    for i in split_after_regex:\n",
    "        include = any(word in i for word in query_list_include)\n",
    "    \n",
    "    \n",
    "    uns = []\n",
    "    if any(w in split_after_regex for w in query_list_include) and not any(w in split_after_regex for w in query_list_exclude):\n",
    "        print(url)\n",
    "        \n",
    "        \n",
    "def iter_query(jobs_dict, query_list_include, query_list_exclude):\n",
    "\n",
    "    urls = [k for k,v in jobs_dict.items()]\n",
    "\n",
    "    matches = []\n",
    "    for u in urls:\n",
    "        matches = query_job_posting(u, query_list_include, query_list_exclude)        \n",
    "        if matches == None:\n",
    "            pass\n",
    "        else:\n",
    "            matches.append(u)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/m/viewjob?jk=15d586a5d4f1d0bd\n",
      "https://www.indeed.com/m/viewjob?jk=cba50f3ef28f77a6\n",
      "https://www.indeed.com/m/viewjob?jk=ff4ce51c9cdf1adc\n",
      "https://www.indeed.com/m/viewjob?jk=e6f6bacb55e74937\n",
      "https://www.indeed.com/m/viewjob?jk=d746d33b2d2c596d\n",
      "https://www.indeed.com/m/viewjob?jk=36798c7ca32ed8d3\n",
      "https://www.indeed.com/m/viewjob?jk=d5ed5321c9c867fd\n",
      "https://www.indeed.com/m/viewjob?jk=cbeaae72bfb2f860\n",
      "https://www.indeed.com/m/viewjob?jk=e25f549a3d98b75b\n",
      "https://www.indeed.com/m/viewjob?jk=ecdfa9fdfee59449\n",
      "https://www.indeed.com/m/viewjob?jk=be5f20ec5a46f151\n",
      "https://www.indeed.com/m/viewjob?jk=c275fbd6e7832d67\n",
      "https://www.indeed.com/m/viewjob?jk=4a96ece7d11fa109\n",
      "https://www.indeed.com/m/viewjob?jk=dc4c827f92698eff\n",
      "https://www.indeed.com/m/viewjob?jk=b1d2ee5e934f8e04\n",
      "https://www.indeed.com/m/viewjob?jk=6f9bdba265edb2cd\n",
      "https://www.indeed.com/m/viewjob?jk=3aa075762cd73fe9\n",
      "https://www.indeed.com/m/viewjob?jk=2c7df1ac2499e3b7\n",
      "https://www.indeed.com/m/viewjob?jk=ae96d2e070635db1\n",
      "https://www.indeed.com/m/viewjob?jk=dbb31355dc63dca5\n",
      "https://www.indeed.com/m/viewjob?jk=d60a286b4be91f37\n",
      "https://www.indeed.com/m/viewjob?jk=a4ae195a5d863219\n",
      "https://www.indeed.com/m/viewjob?jk=9a0165a45aac91f7\n",
      "https://www.indeed.com/m/viewjob?jk=3ccac7665681bffe\n",
      "https://www.indeed.com/m/viewjob?jk=656182b8702d17f4\n",
      "https://www.indeed.com/m/viewjob?jk=fe2eafc9d29dcb08\n",
      "https://www.indeed.com/m/viewjob?jk=8e5c115276c107be\n",
      "https://www.indeed.com/m/viewjob?jk=3eb5fa4bd75bd351\n",
      "https://www.indeed.com/m/viewjob?jk=622a1204304c05ca\n",
      "https://www.indeed.com/m/viewjob?jk=30e525e0a1dd225e\n",
      "https://www.indeed.com/m/viewjob?jk=f79b712ad8dfbc36\n"
     ]
    }
   ],
   "source": [
    "# finds all jobs that have python but not javascript in description \n",
    "query_list_include = ['python']\n",
    "query_list_exclude = ['javascript']\n",
    "\n",
    "iter_query(job_dict, query_list_include,  query_list_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.indeed.com/m/viewjob?jk=622a1204304c05ca'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ur = 'https://www.indeed.com/m/viewjob?jk=c41178aee2681cc3'\n",
    "\n",
    "d = 'C:\\\\Users\\\\Anthony\\\\Documents\\\\DB\\\\indeed25.sqlite'\n",
    "\n",
    "applied_jobs(ur, d, new_db=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
