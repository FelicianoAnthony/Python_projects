{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import sqlite3\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "import urllib.request\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def setup_webdriver(): \n",
    "    \n",
    "    # set up wen driver\n",
    "    chromedriver = r\"C:\\Users\\Anthony\\Desktop\\chromedriver.exe\"\n",
    "    os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    return driver\n",
    "\n",
    "\n",
    "def create_soup(url):\n",
    "    ''' create bs4 object '''\n",
    "    \n",
    "    r = requests.get(url, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36\"\n",
    "    })\n",
    "    return BeautifulSoup(r.content, \"html5lib\")\n",
    "\n",
    "\n",
    "def scrape_job_links(url):\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # get hrefs ONLY for non- header and footer job postings \n",
    "    lst1= []\n",
    "    for div in soup.find_all('div', {'class':' row result'}):\n",
    "        links = div.find('a')['href']\n",
    "        #print(links)\n",
    "        lst1.append(links)\n",
    "\n",
    "    # last row has different class name\n",
    "    for div in soup.find_all('div', {'class':'lastRow row result'}):\n",
    "        links = div.find('a')['href']\n",
    "        #print(div)\n",
    "        lst1.append(links)\n",
    "    \n",
    "    links = ['https://www.indeed.com' + i for i in lst1]\n",
    "    return links\n",
    "\n",
    "\n",
    "def next_page_url(url):\n",
    "    ''' returns the url for the next page on an indeed job search page '''\n",
    "    \n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # this creates a list of \"Results Page\" at bottom of screen... last url will always be next page \n",
    "    next_pages_urls = []\n",
    "    for i in soup.find_all(attrs={'class': 'pagination'}):\n",
    "        a_tags = i.find_all('a')\n",
    "        for a in a_tags:\n",
    "            next_pages = 'https://www.indeed.com' + a['href']\n",
    "            next_pages_urls.append(next_pages)\n",
    "\n",
    "    return next_pages_urls[-1]\n",
    "\n",
    "\n",
    "def filter_links(links_lst):\n",
    "    '''separates scraped indeed links based on whether they redirect you to \n",
    "       internal indeed job posts or external company website '''\n",
    "    \n",
    "    indeed_links = []\n",
    "    non_indeed_links = []\n",
    "    for i in links_lst:\n",
    "        if i.startswith('https://www.indeed.com/rc'):\n",
    "            non_indeed_links.append(i)\n",
    "            pass\n",
    "        else:\n",
    "            indeed_links.append(i)\n",
    "            \n",
    "    return indeed_links, non_indeed_links\n",
    "\n",
    "\n",
    "def bottom_scroll(webdriver):\n",
    "    ''''scroll to bottom of page--- w/ page search pages -- not individual job postings'''\n",
    "    element=webdriver.find_element_by_xpath('//*[@id=\"resultsCol\"]/div[18]')\n",
    "    return element.location_once_scrolled_into_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def indeed_scraper(job_name, job_location, int_pages_to_search): \n",
    "    '''this combined everything '''\n",
    "    \n",
    "\n",
    "    driver = setup_webdriver()\n",
    "    \n",
    "    # set website to scrape and query \n",
    "    driver.get(\"https://www.indeed.com/\")\n",
    "    driver.find_element_by_id('what').clear()\n",
    "    driver.find_element_by_id('what').send_keys(job_name)\n",
    "    driver.find_element_by_id('where').clear()\n",
    "    driver.find_element_by_id('where').send_keys(job_location)\n",
    "    driver.find_element_by_id('fj').click()\n",
    "    \n",
    "    # get current url and pass into bs4\n",
    "    this_url = driver.current_url\n",
    "    date_soup = create_soup(this_url)\n",
    "    \n",
    "    # sort by date \n",
    "    date_button = date_soup.find(attrs={'class': 'no-wrap'})\n",
    "    date_button2 = date_button.find('a')['href']\n",
    "    sort_by_date = 'https://www.indeed.com' + date_button2\n",
    "    \n",
    "    # pass url to driver & get current url to scrape links \n",
    "    driver.get(sort_by_date)\n",
    "    \n",
    "    # scrape em\n",
    "    all_job_links = []\n",
    "    for i in range(int(int_pages_to_search)):\n",
    "        # get current url\n",
    "        driver.get(driver.current_url)\n",
    "        \n",
    "        # get job links & append to list\n",
    "        job_links = scrape_job_links(driver.current_url)\n",
    "        all_job_links.extend(job_links)\n",
    "        \n",
    "        # scroll to the bottom of screen\n",
    "        bottom_scroll(driver)\n",
    "        \n",
    "        # go to next page\n",
    "        url_next = next_page_url(driver.current_url)\n",
    "        driver.get(url_next)\n",
    "    return all_job_links\n",
    "\n",
    "# scrape text from only the indeed postings -- text stored in paragraph and list tags \n",
    "\n",
    "def job_description_to_query(url, keywords): \n",
    "    ''' job descriptions have p and li tags -- when scraped theyre list of lists --\n",
    "       flatten lists separetely, combined into 1 list, make everything lowercase &\n",
    "       remove special characters '''\n",
    "    \n",
    "    # create soup\n",
    "    soup = create_soup(url)\n",
    "    \n",
    "    # get all p tags\n",
    "    para_lst = []\n",
    "    for p in soup.find_all('p'):\n",
    "        if 'style' in p.attrs:\n",
    "            pass\n",
    "        else: \n",
    "            p_text = p.get_text()\n",
    "            para_lst.append(p_text)\n",
    "\n",
    "    # p tags is a list of lists so we need to flatten it\n",
    "    new_para_lst=[]\n",
    "    para_len = len(para_lst)\n",
    "\n",
    "    # split items in list then iterate over each list and append to new list \n",
    "    para_split = [i.split() for i in para_lst]\n",
    "    for x in range(para_len):\n",
    "        for i in para_split[x]:\n",
    "            new_para_lst.append(i)\n",
    "\n",
    "    # get all li tags \n",
    "    lists = soup.find_all('li')\n",
    "    lists_text = [i.get_text() for i in lists]        \n",
    "\n",
    "    # li tag is a list of lists so we need to flatten it \n",
    "    new_li_tag_lst = []\n",
    "    lists_len = len(lists_text)\n",
    "\n",
    "    lists_split = [i.split() for i in lists_text]\n",
    "    for x in range(lists_len):\n",
    "        for i in lists_split[x]:\n",
    "            new_li_tag_lst.append(i)\n",
    "\n",
    "    # get job title and company name \n",
    "    job_name = soup.find('b', attrs={'class': 'jobtitle'}).get_text()\n",
    "    company = soup.find('span', attrs={'class':'company'}).get_text()\n",
    "    \n",
    "    # combined cleaned lists \n",
    "    job_description = new_li_tag_lst + new_para_lst\n",
    "    \n",
    "    # format lists by remove any non-alphanum char, spaces, and make everything lowercase\n",
    "    clean_job_description = []\n",
    "    for i in job_description:\n",
    "        remove_spec_chars = re.sub('[^A-Za-z0-9]+', '', i) # removes anything thats not a letter or number \n",
    "        lowercase = remove_spec_chars.lower()\n",
    "        if lowercase: # remove spaces in list \n",
    "            clean_job_description.append(lowercase)\n",
    "    \n",
    "    #keywords = ['python', 'pandas']\n",
    "    for i in clean_job_description:\n",
    "        if any(word in i for word in keywords):\n",
    "            return url\n",
    "        \n",
    "\n",
    "def view_jobs(jobs_filtered, lst_of_keywords, sleep_time):\n",
    "    ''' given a list of ONLY INDEED JOB LINKS -- filters by keyword,\n",
    "        opens job post in new tab, gives user time to read, opens next \n",
    "        job post in new tab in same window -- all tabs that remain open \n",
    "        are the posts user deemed relevant '''\n",
    "\n",
    "    driver = setup_webdriver()\n",
    "\n",
    "    for j in jobs_filtered:\n",
    "        job_url =job_description_to_query(j, lst_of_keywords)\n",
    "        concat_url = 'window.open(\"' + job_url + '\",\"_blank\");'\n",
    "        driver.execute_script(concat_url)\n",
    "        time.sleep(int(sleep_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# search indeed for job name and location "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jobs = indeed_scraper('python', 'New York, NY', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# separate links based on whether indeed postings or links take you to company website -- indeed posts are easier to search job description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jobs_filt = filter_links(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# take indeed links & see if keywords are in job description...\n",
    "# launch a new tab in same window for every job posting...\n",
    "# program sleeps to allow time to read job posting...\n",
    "# if user not interested in job, x it out...\n",
    "# and new posting will appear in new tab in same window\n",
    "# all leftover posts are the ones user is interested in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7de2f0351a96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mview_jobs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobs_filt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-35c9e8768d14>\u001b[0m in \u001b[0;36mview_jobs\u001b[1;34m(jobs_filtered, lst_of_keywords, sleep_time)\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mconcat_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'window.open(\"'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mjob_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\",\"_blank\");'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msleep_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "view_jobs(jobs_filt[0], ['python', 'training'] 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_db_table(full_path_to_db):\n",
    "    \n",
    "    conn = sqlite3.connect(full_path_to_db)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE indeed_jobs\n",
    "        (id integer primary key, data,\n",
    "        url text, \n",
    "        company_name text, \n",
    "        job_title text)''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def post_to_db(jobs_filt, full_path_to_db, first_run=None):\n",
    "    \n",
    "    driver = setup_webdriver()\n",
    "    \n",
    "    if first_run:\n",
    "        create_db_table(full_path_to_db)\n",
    "    \n",
    "    conn = sqlite3.connect(full_path_to_db)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    tup_db = []\n",
    "    for url in jobs_filt:\n",
    "        driver.get(url)\n",
    "        soup = create_soup(driver.current_url)\n",
    "        job_name = soup.find('b', attrs={'class': 'jobtitle'}).get_text()\n",
    "        company = soup.find('span', attrs={'class':'company'}).get_text()\n",
    "        tup = job_name, company, driver.current_url\n",
    "        tup_db.append(tup)\n",
    "        \n",
    "        #c.execute(\"insert or ignore into indeed_jobs (url, company_name, job_title) values (?, ?, ?)\",\n",
    "            #(driver.current_url, company, job_name))\n",
    "        #conn.commit()\n",
    "        \n",
    "        c.execute('SELECT * FROM indeed_jobs WHERE (url=? AND company_name=? AND job_title=?)', (driver.current_url, company, job_name))\n",
    "        entry = c.fetchone()\n",
    "\n",
    "        if entry is None:\n",
    "            c.execute(\"insert or ignore into indeed_jobs (url, company_name, job_title) values (?, ?, ?)\",\n",
    "                (driver.current_url, company, job_name))\n",
    "            conn.commit()\n",
    "            \n",
    "            \n",
    "            job_url =job_description_to_query(driver.current_url, ['python', 'training'])\n",
    "            print ('New entry added', '\\n', job_url)\n",
    "            concat_url = 'window.open(\"' + job_url + '\",\"_blank\");'\n",
    "            driver.execute_script(concat_url)\n",
    "            time.sleep(5)\n",
    "\n",
    "        else:\n",
    "            print ('Entry found')\n",
    "        \n",
    "    #conn.close()\n",
    "        \n",
    "    return tup_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New entry added\n",
      "https://www.indeed.com/cmp/VICE-Media-Inc./jobs/Data-Engineer-5bc130fad4ba954c\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/Neo-Tech-Solutions-Inc/jobs/Python-Engineer-Senior-7aaa368bcb190f7b\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/IITS/jobs/Oop-754be9f354c37f91\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/OperationIT.COM/jobs/Trade-Support-Analyst-0df841327b129976\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/THEMESOFT/jobs/Devop-Cloud-ac794be67e41f0a3\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/US-Engineering-Technical-Services/jobs/Image-Processing-Engineer-93ab7d98657e58f7\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/Access-Staffing-LLC/jobs/Web-Architect-9c07bc52cbe296f8\n",
      "New entry added\n",
      "https://www.indeed.com/cmp/Intelletec/jobs/Ruby-Rail-Engineer-7d5e112d9ae5fe2b\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-4228252ddf37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdb_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mr'C:\\Users\\Anthony\\Documents\\db\\indeed12.sqlite'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpost_to_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjobs_filt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfirst_run\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-49-1aa6cdc570a9>\u001b[0m in \u001b[0;36mpost_to_db\u001b[1;34m(jobs_filt, full_path_to_db, first_run)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mconcat_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'window.open(\"'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mjob_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\",\"_blank\");'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "db_path = r'C:\\Users\\Anthony\\Documents\\db\\indeed12.sqlite'\n",
    "\n",
    "post_to_db(jobs_filt[0], db_path, first_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scroll(webdriver):\n",
    "    \n",
    "    # scroll to bottom of page\n",
    "    element=webdriver.find_element_by_xpath('//*[@id=\"resultsCol\"]/div[18]')\n",
    "    return element.location_once_scrolled_into_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smooth_scrolling():\n",
    "\n",
    "    driver= setup_webdriver()\n",
    "    \n",
    "    # get specifics \n",
    "    driver.get(\"https://www.indeed.com/\")\n",
    "    driver.find_element_by_id('what').clear()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('what').send_keys('python developer')\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('where').clear()\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('where').send_keys('New York, NY')\n",
    "    time.sleep(1)\n",
    "    driver.find_element_by_id('fj').click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    driver.find_element_by_id('prime-popover-x').click()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # scrape jobs from that url \n",
    "    job_links = scrape_job_links(driver.current_url)\n",
    "    # filter to only indeed jobs\n",
    "    jobs_filt = filter_links(job_links)\n",
    "    \n",
    "    # start at a good xpath and count up div tags that appear in every page to 'smooth' scroll\n",
    "    count = 0\n",
    "    for i in range(2):\n",
    "        for j in jobs_filt:\n",
    "            driver.get(j)\n",
    "            time.sleep(2)\n",
    "            count+=1\n",
    "            #print(count, j)\n",
    "            str_ = str(count)\n",
    "\n",
    "            #concat_div = '//*[@id=\"resultsCol\"]/div[' + str_ + ']'\n",
    "            try: \n",
    "                concat_div = '//*[@id=\"job-content\"]/tbody/tr/td[1]/table/tbody/tr/td/div[2]/div[2]/div[' + str_ + ']'\n",
    "                #concat_div = '//*[@id=\"job-content\"]/tbody/tr/td[1]/table/tbody/tr/td/div[2]/div[' + str_ + ']'\n",
    "                element=driver.find_element_by_xpath(concat_div)\n",
    "                element.location_once_scrolled_into_view\n",
    "                time.sleep(2)\n",
    "                driver.get(j)\n",
    "                print(j)\n",
    "            except:\n",
    "                browser.get(j)\n",
    "                print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.indeed.com/company/NYTP/jobs/Python-Engineer-8578a5b36c166821?fccid=ccff84ac3dd19bb9\n",
      "https://www.indeed.com/company/RAPS-consulting/jobs/Senior-Python-Developer-21d5e8cd39ab722f?fccid=9b7a3860892e2fcd\n",
      "https://www.indeed.com/company/Kasisto/jobs/Front-End-Software-Engineer-e8ee0083bfbe3831?fccid=521ca7ccc49e32d5\n",
      "https://www.indeed.com/company/Chase-Dream-LLC/jobs/Software-Engineer-Python-98563184e467ab84?fccid=ed0eadb761b45e00\n",
      "https://www.indeed.com/company/LIS-Solutions/jobs/Python-Developer-a424fc091c993864?fccid=58aef85eed32eea8\n",
      "https://www.indeed.com/company/BlindData/jobs/Software-Engineer-010d7c76f5d1fa19?fccid=55962a85574fa94f\n",
      "https://www.indeed.com/company/NYTP/jobs/Python-Engineer-8578a5b36c166821?fccid=ccff84ac3dd19bb9\n",
      "https://www.indeed.com/company/RAPS-consulting/jobs/Senior-Python-Developer-21d5e8cd39ab722f?fccid=9b7a3860892e2fcd\n",
      "https://www.indeed.com/company/Kasisto/jobs/Front-End-Software-Engineer-e8ee0083bfbe3831?fccid=521ca7ccc49e32d5\n",
      "https://www.indeed.com/company/Chase-Dream-LLC/jobs/Software-Engineer-Python-98563184e467ab84?fccid=ed0eadb761b45e00\n",
      "https://www.indeed.com/company/LIS-Solutions/jobs/Python-Developer-a424fc091c993864?fccid=58aef85eed32eea8\n",
      "https://www.indeed.com/company/BlindData/jobs/Software-Engineer-010d7c76f5d1fa19?fccid=55962a85574fa94f\n"
     ]
    }
   ],
   "source": [
    "smooth_scrolling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
